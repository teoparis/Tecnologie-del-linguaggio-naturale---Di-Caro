{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc6c91c-de97-4491-98a4-25202da67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef5d635-95ad-4fd8-84e7-29a40503a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "# Union of the pre-processed words of the definitions and terms from the examples in WN for a sense.\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88648d7-c858-4786-b49f-24068483d4ab",
   "metadata": {},
   "source": [
    "#### First exercize\n",
    "It calculates average definition lenght for each section (nouns, verbs, adjectives and adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae19b6e-e10a-4b85-97c3-8cec8e5378ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def avg_len_section_definitons():\n",
    "    pos_tag_list = ['n', 'v', 'a', 'r']\n",
    "    average_lenghts = []\n",
    "\n",
    "    for pos_tag in pos_tag_list:\n",
    "        synsets_lenght = []\n",
    "        for synset in list(wn.all_synsets(pos_tag)):\n",
    "            synsets_lenght.append(len(synset.definition().split(\" \")))\n",
    "        average_lenghts.append((pos_tag, mean(synsets_lenght)))\n",
    "\n",
    "    print(\"\\n\", average_lenghts, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156e046d-1568-48a8-b0a1-24c50baeff31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [('n', 11.470035925226815), ('v', 6.146655044672042), ('a', 7.238433575677462), ('r', 5.028169014084507)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_len_section_definitons()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a398d-e50c-41be-8fd5-1129d23a25ba",
   "metadata": {},
   "source": [
    "#### Second exercize\n",
    "The variation of the length along the path of the hyperonyms that lead from a given synset to its root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fdd85c8-d997-466a-94c5-d1cb641669fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hypernym_paths(word):\n",
    "    \n",
    "    def_lens = []\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "\n",
    "\n",
    "        single_path = []\n",
    "        \n",
    "        hyp_path = syn.hypernym_paths()\n",
    "        \n",
    "        for i in range (0, len(hyp_path[0])):\n",
    "            \n",
    "            single_path.append((hyp_path[0][i],len((hyp_path[0][i].definition()).split())))\n",
    "\n",
    "        print(single_path)\n",
    "        print()\n",
    "        def_lens.append(single_path)\n",
    "\n",
    "    return def_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb7ef8b-82e9-4774-b7b4-83ffb7bf5067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('attribute.n.02'), 9), (Synset('trait.n.01'), 7), (Synset('character.n.03'), 18), (Synset('spirit.n.03'), 9), (Synset('courage.n.01'), 15)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('matter.n.03'), 7), (Synset('substance.n.01'), 11), (Synset('material.n.01'), 12), (Synset('paper.n.01'), 15)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('communication.n.02'), 12), (Synset('written_communication.n.01'), 10), (Synset('writing.n.02'), 24), (Synset('essay.n.01'), 6), (Synset('composition.n.08'), 8)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('medium.n.01'), 9), (Synset('print_media.n.01'), 6), (Synset('press.n.02'), 16), (Synset('newspaper.n.01'), 14)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('medium.n.01'), 9), (Synset('paper.n.04'), 5)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('communication.n.02'), 12), (Synset('expressive_style.n.01'), 25), (Synset('writing_style.n.01'), 7), (Synset('prose.n.01'), 6), (Synset('nonfiction.n.01'), 6), (Synset('article.n.01'), 9), (Synset('paper.n.05'), 11)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('group.n.01'), 9), (Synset('social_group.n.01'), 5), (Synset('organization.n.01'), 7), (Synset('enterprise.n.02'), 6), (Synset('business.n.01'), 11), (Synset('firm.n.01'), 14), (Synset('publisher.n.01'), 6), (Synset('newspaper.n.02'), 6)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('creation.n.02'), 10), (Synset('product.n.02'), 11), (Synset('newspaper.n.03'), 11)]\n",
      "\n",
      "[(Synset('cover.v.01'), 9), (Synset('paper.v.01'), 3)]\n",
      "\n",
      "[(Synset('cover.v.01'), 9), (Synset('wallpaper.v.01'), 3)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('attribute.n.02'), 9), (Synset('state.n.02'), 10), (Synset('feeling.n.01'), 7), (Synset('emotion.n.01'), 3), (Synset('fear.n.01'), 20), (Synset('apprehension.n.01'), 4)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('cognition.n.01'), 9), (Synset('process.n.02'), 14), (Synset('higher_cognitive_process.n.01'), 13), (Synset('knowing.n.01'), 6), (Synset('understanding.n.01'), 7)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('cognition.n.01'), 9), (Synset('content.n.05'), 12), (Synset('belief.n.01'), 6), (Synset('expectation.n.01'), 8), (Synset('apprehension.n.03'), 2)]\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('event.n.01'), 9), (Synset('act.n.02'), 8), (Synset('acquiring.n.01'), 5), (Synset('capture.n.01'), 9), (Synset('apprehension.n.04'), 8)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('implement.n.01'), 12), (Synset('sharpener.n.01'), 14)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \",word)\n",
    "    all_hypernym_paths(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb97c5f-143d-4c46-af14-cdf072f35c26",
   "metadata": {},
   "source": [
    "#### Third exercize\n",
    "Distance from the word's root and words within the definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c60b5d8-8368-4c47-99dd-a33b349eff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_root(synset):\n",
    "    return (min([len(path) for path in synset.hypernym_paths()]))\n",
    "\n",
    "def distance_root(word):\n",
    "    \n",
    "    output = dict()\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "        \n",
    "        actual_syn_dis = calculate_distance_root(syn)\n",
    "        output[syn] = {word :actual_syn_dis} \n",
    "                \n",
    "        syn_definition_processed = pre_processing(syn.definition())\n",
    "        for def_word in syn_definition_processed:\n",
    "            min_dis=9999\n",
    "            for def_syn in wn.synsets(def_word):\n",
    "                if min_dis > calculate_distance_root(def_syn):\n",
    "                    min_dis = calculate_distance_root(def_syn)\n",
    "                    output[syn].update({def_word : min_dis})\n",
    "                \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332db380-cb06-4da6-bdfd-29fd9080073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "{Synset('courage.n.01'): {'Courage': 7, 'quality': 1, 'spirit': 5, 'enable': 2, 'face': 1, 'danger': 4, 'pain': 4, 'fear': 1}}\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "{Synset('paper.n.01'): {'Paper': 6, 'material': 1, 'cellulose': 9, 'pulp': 3, 'derive': 1, 'wood': 6, 'rag': 2, 'grass': 2}, Synset('composition.n.08'): {'Paper': 7, 'essay': 2, 'write': 1, 'assignment': 6}, Synset('newspaper.n.01'): {'Paper': 10, 'daily': 1, 'weekly': 1, 'publication': 4, 'folded': 1, 'sheet': 2, 'news': 6, 'article': 5, 'advertisement': 6}, Synset('paper.n.04'): {'Paper': 8, 'medium': 1, 'write': 1, 'communication': 3}, Synset('paper.n.05'): {'Paper': 9, 'scholarly': 1, 'article': 5, 'describe': 1, 'result': 2, 'observation': 7, 'hypothesis': 6}, Synset('newspaper.n.02'): {'Paper': 10, 'business': 5, 'firm': 1, 'publish': 2, 'newspaper': 7}, Synset('newspaper.n.03'): {'Paper': 8, 'physical': 1, 'object': 2, 'product': 6, 'newspaper': 7, 'publisher': 7}, Synset('paper.v.01'): {'Paper': 2, 'cover': 1, 'paper': 2}, Synset('wallpaper.v.01'): {'Paper': 2, 'cover': 1, 'wallpaper': 2}}\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "{Synset('apprehension.n.01'): {'Apprehension': 8, 'fearful': 1, 'expectation': 6, 'anticipation': 7}, Synset('understanding.n.01'): {'Apprehension': 8, 'cognitive': 1, 'condition': 3, 'understand': 1}, Synset('apprehension.n.03'): {'Apprehension': 8, 'painful': 1, 'expectation': 6}, Synset('apprehension.n.04'): {'Apprehension': 8, 'apprehend': 2, 'criminal': 1}}\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "{Synset('sharpener.n.01'): {'Sharpener': 8, 'implement': 2, 'edge': 2, 'point': 2, 'sharper': 1}}\n"
     ]
    }
   ],
   "source": [
    "for word in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \",word)\n",
    "    print(distance_root(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a7afe-0b0b-4ab8-903e-63839a16a3f0",
   "metadata": {},
   "source": [
    "#### Fourth exercize\n",
    "Calculate similarity scores between hypernyms and hyponyms definitions with concept's definition. We will use Sentence Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53bcb3da-320a-46bf-904d-1616f7b34952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "'''\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bleu import multi_list_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "'''\n",
    "\n",
    "def definition_overlap(word):\n",
    "    \n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "        \n",
    "        '''\n",
    "        rouge = Rouge()\n",
    "        bleu_count = 0\n",
    "        f_count = 0\n",
    "        '''\n",
    "        \n",
    "        embedding_sim = 0\n",
    "        \n",
    "        actual_def_processed = syn.definition()\n",
    "        \n",
    "        print (\"\\n\\nDefinition of\", syn,  \"=\", actual_def_processed)\n",
    "        print ()\n",
    "        \n",
    "        hyper_list = syn.hypernyms()\n",
    "        \n",
    "        for hy in hyper_list:\n",
    "            \n",
    "            hy_def = hy.definition()\n",
    "            hyper_def_list = []\n",
    "            \n",
    "            hyper_def_list.append(actual_def_processed)\n",
    "            hyper_def_list.append(hy_def)\n",
    "            hyper_def_list_emb = model.encode(hyper_def_list)\n",
    "            \n",
    "            embedding_sim += 1 - distance.cosine(hyper_def_list_emb[0], hyper_def_list_emb[1])\n",
    "            \n",
    "            '''bleu_count += sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0))\n",
    "            #print(\"BLEU score: \", sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0)))\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(' '.join(hy_def), ' '.join(actual_def_processed))\n",
    "            #print(\"Rogue scores: \", rouge_scores)\n",
    "            f_count += rouge_scores[0]['rouge-1']['f']'''\n",
    "\n",
    "        if (len(hyper_list)!=0):\n",
    "            print (\"Average similarity for hypernyms:\",  embedding_sim/len(hyper_list))    \n",
    "            \n",
    "        '''if (len(hyper_list) != 0):\n",
    "            print (\"Bleu score for hypernyms (1-gram):\", bleu_count / len(hyper_list))\n",
    "            print (\"Rogue f1 for hypernyms (1-gram):\", f_count / len(hyper_list))\n",
    "        else:\n",
    "            print(\"No hypernyms\")'''\n",
    "\n",
    "               \n",
    "        print ()\n",
    "        \n",
    "        '''bleu_count = 0\n",
    "        f_count = 0'''\n",
    "        embedding_sim = 0\n",
    "\n",
    "        hypo_list = syn.hyponyms()\n",
    "               \n",
    "        for hy in hypo_list:\n",
    "            hy_def = hy.definition()\n",
    "            \n",
    "            \n",
    "            hypo_def_list = []\n",
    "            \n",
    "            hypo_def_list.append(hy_def)\n",
    "            hypo_def_list.append(actual_def_processed)\n",
    "            \n",
    "            hypo_def_list_em = model.encode(hypo_def_list)\n",
    "            \n",
    "            embedding_sim += 1 - distance.cosine(hypo_def_list_em[0], hypo_def_list_em[1])\n",
    "\n",
    "\n",
    "            \n",
    "            '''bleu_count += sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0))\n",
    "            #print(\"BLEU score: \", sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0)))\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(' '.join(hy_def), ' '.join(actual_def_processed))\n",
    "            #print(\"Rogue scores: \", rouge_scores)\n",
    "            f_count += rouge_scores[0]['rouge-1']['f']'''\n",
    "            \n",
    "        if (len(hypo_list)!=0):\n",
    "            print (\"Average similarity for hyponyms:\",  embedding_sim/len(hypo_list))\n",
    "\n",
    "        '''if (len(hypo_list) != 0):\n",
    "            print (\"Bleu score for hyponyms (1-gram):\", bleu_count / len(hypo_list))\n",
    "            print (\"Rogue f1 for hyponyms (1-gram):\", f_count / len(hypo_list))\n",
    "        else:\n",
    "            print(\"No hyponyms\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6b0e43-226e-49c3-b8a6-fae5e1ee8801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "\n",
      "\n",
      "Definition of Synset('courage.n.01') = a quality of spirit that enables you to face danger or pain without showing fear\n",
      "\n",
      "Average similarity for hypernyms: 0.559572160243988\n",
      "\n",
      "Average similarity for hyponyms: 0.655585263456617\n",
      "None\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "\n",
      "\n",
      "Definition of Synset('paper.n.01') = a material made of cellulose pulp derived mainly from wood or rags or certain grasses\n",
      "\n",
      "Average similarity for hypernyms: 0.4182778298854828\n",
      "\n",
      "Average similarity for hyponyms: 0.544027472651282\n",
      "\n",
      "\n",
      "Definition of Synset('composition.n.08') = an essay (especially one written as an assignment)\n",
      "\n",
      "Average similarity for hypernyms: 0.6943868398666382\n",
      "\n",
      "Average similarity for hyponyms: 0.6131163835525513\n",
      "\n",
      "\n",
      "Definition of Synset('newspaper.n.01') = a daily or weekly publication on folded sheets; contains news and articles and advertisements\n",
      "\n",
      "Average similarity for hypernyms: 0.7504271268844604\n",
      "\n",
      "Average similarity for hyponyms: 0.6325632035732269\n",
      "\n",
      "\n",
      "Definition of Synset('paper.n.04') = a medium for written communication\n",
      "\n",
      "Average similarity for hypernyms: 0.8130615949630737\n",
      "\n",
      "\n",
      "\n",
      "Definition of Synset('paper.n.05') = a scholarly article describing the results of observations or stating hypotheses\n",
      "\n",
      "Average similarity for hypernyms: 0.5744180083274841\n",
      "\n",
      "\n",
      "\n",
      "Definition of Synset('newspaper.n.02') = a business firm that publishes newspapers\n",
      "\n",
      "Average similarity for hypernyms: 0.8628333806991577\n",
      "\n",
      "\n",
      "\n",
      "Definition of Synset('newspaper.n.03') = the physical object that is the product of a newspaper publisher\n",
      "\n",
      "Average similarity for hypernyms: 0.5419683456420898\n",
      "\n",
      "\n",
      "\n",
      "Definition of Synset('paper.v.01') = cover with paper\n",
      "\n",
      "Average similarity for hypernyms: 0.724046528339386\n",
      "\n",
      "\n",
      "\n",
      "Definition of Synset('wallpaper.v.01') = cover with wallpaper\n",
      "\n",
      "Average similarity for hypernyms: 0.6008650660514832\n",
      "\n",
      "None\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "\n",
      "\n",
      "Definition of Synset('apprehension.n.01') = fearful expectation or anticipation\n",
      "\n",
      "Average similarity for hypernyms: 0.7708446383476257\n",
      "\n",
      "Average similarity for hyponyms: 0.8047566533088684\n",
      "\n",
      "\n",
      "Definition of Synset('understanding.n.01') = the cognitive condition of someone who understands\n",
      "\n",
      "Average similarity for hypernyms: 0.679866373538971\n",
      "\n",
      "Average similarity for hyponyms: 0.7183353006839752\n",
      "\n",
      "\n",
      "Definition of Synset('apprehension.n.03') = painful expectation\n",
      "\n",
      "Average similarity for hypernyms: 0.45832911133766174\n",
      "\n",
      "\n",
      "\n",
      "Definition of Synset('apprehension.n.04') = the act of apprehending (especially apprehending a criminal)\n",
      "\n",
      "Average similarity for hypernyms: 0.5665028095245361\n",
      "\n",
      "None\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "\n",
      "\n",
      "Definition of Synset('sharpener.n.01') = any implement that is used to make something (an edge or a point) sharper\n",
      "\n",
      "Average similarity for hypernyms: 0.7330511212348938\n",
      "\n",
      "Average similarity for hyponyms: 0.6331739306449891\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for word in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \",word)\n",
    "    print(definition_overlap(word))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "687777cbfed9bcee7b08ad56f15c17d8908988aca6cd0ae5484f3a9e4851bb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
