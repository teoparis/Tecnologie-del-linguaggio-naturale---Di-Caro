{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07bbbbb-d2c3-4083-93de-730792971862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9769658f-8fef-46de-bea5-aa221e62b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "# return row of the .csv file\n",
    "def get_rows(readCSV):\n",
    "    for row in readCSV: return row\n",
    "\n",
    "# Union of the pre-processed words of the definitions and terms from the examples in WN for a sense.\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8624fb4-31eb-4814-bb6b-065c5c60e093",
   "metadata": {},
   "source": [
    "Take in input file of definition .csv. in output return a dictionary in which each word ('Courage', 'Paper', 'Apprehension', 'Sharpener') is associated with a set of definitions\n",
    "\n",
    "Definitions are pre-processed before being entered into the dictionary.\n",
    "\n",
    "In output we will have a list of lists per word in which each list represents a definition in the form of BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0953bfd0-6ec4-4a3e-9f40-54a7705a1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_definitions(file):\n",
    "    \"\"\"\n",
    "    :param file: link to csv file\n",
    "    :return: a dictionary as {word: {definition1,...}}\n",
    "    \"\"\"\n",
    "    readCSV = csv.reader(file, delimiter=',')\n",
    "    \n",
    "    # get list of words to analize\n",
    "    words = get_rows(readCSV)[1:]\n",
    "\n",
    "    definitions_words = dict()\n",
    "\n",
    "    for row in readCSV:\n",
    "        for index, definition in enumerate(row):\n",
    "            # check if the definition is empty\n",
    "            if definition:  \n",
    "                if index > 0:\n",
    "                    word = words[index - 1]\n",
    "                    if word not in definitions_words.keys():\n",
    "                        definitions_words[word] = [pre_processing(definition)]\n",
    "                    else:\n",
    "                        definitions_words[word].append(pre_processing(definition))\n",
    "    return definitions_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9cdcf9-f2f6-41eb-952d-0b3bd0419e1e",
   "metadata": {},
   "source": [
    "Calculate cosine similarity scores between word's definitions.\n",
    "\n",
    "Calculate similarity between two vectors which correspond presence or absence of certain word (for each word in definition 1 and definition 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0d3362-a665-4e5a-bcec-88b746146f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(def1, def2):\n",
    "    \"\"\"\n",
    "    :param def1: text of definiton 1\n",
    "    :param def2: text of definiton 2\n",
    "    \"\"\"\n",
    "    vector_def1 = []\n",
    "    vector_def2 = []\n",
    "    \n",
    "    # Obtain a vector indicating precense or absence of all words\n",
    "    both_def = list(set(def1) | set(def2))\n",
    "    for word in both_def:\n",
    "        if word in def1:\n",
    "            vector_def1.append(1)\n",
    "        else:\n",
    "            vector_def1.append(0)\n",
    "        if word in def2:\n",
    "            vector_def2.append(1)\n",
    "        else:\n",
    "            vector_def2.append(0)\n",
    "\n",
    "    c = 0\n",
    "    for i in range(len(both_def)):\n",
    "        c += vector_def1[i] * vector_def2[i]\n",
    "    cosine_score = c / float((sum(vector_def1) * sum(vector_def2)) ** 0.5)\n",
    "    return cosine_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba29705-f6e1-4ed3-83e5-9a4ef75c4f5f",
   "metadata": {},
   "source": [
    "Compute results doing an average between all couple of definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ffc45e-3e5e-4984-a9bc-053b1db92739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(definitions_words):\n",
    "    results = dict()\n",
    "    for word in definitions_words.keys():\n",
    "        definitions = definitions_words[word]\n",
    "        \n",
    "        avg_similarity = 0\n",
    "        count = 0\n",
    "        for def1 in definitions:\n",
    "            for def2 in definitions:\n",
    "                if not def1 == def2:\n",
    "                    avg_similarity += cosine_sim(def1, def2)\n",
    "                    count += 1\n",
    "        \n",
    "        results[word] = avg_similarity / count\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97932fa7-8def-4d19-b19e-5e08cc5ccfe5",
   "metadata": {},
   "source": [
    "Calculate words wich appears at least in 50 per cent of definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73b5509-93f2-4e55-9ed3-200664d33a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_words(definitions):\n",
    "    BoW = set([word for definition in definitions for word in definition])\n",
    "\n",
    "    freq_words = dict()\n",
    "\n",
    "    # count how times appears a certain word\n",
    "    for word in BoW:\n",
    "        for definition in definitions:\n",
    "            if word in definition:\n",
    "                if word not in freq_words.keys():\n",
    "                    freq_words[word] = 1\n",
    "                else:\n",
    "                    count = freq_words[word] + 1\n",
    "                    freq_words[word] = count\n",
    "\n",
    "    # takes only words wich appears at least in 50 per cent of definitions\n",
    "    \n",
    "    most_frequent_words = []\n",
    "    \n",
    "    for word in freq_words.keys():\n",
    "        if freq_words[word] >= (0.5 * len(definitions)):\n",
    "            most_frequent_words.append(word)\n",
    "\n",
    "    return most_frequent_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b967a24-9f4b-430f-ae3b-2cd076ccfa29",
   "metadata": {},
   "source": [
    "Compute average cosine similarity between all definitions of a concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f00f5da8-0f2d-407d-9d57-41fccdac424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average definition's cosine similarity: \n",
      "{'Courage': 0.21054727554969985, 'Paper': 0.29258850377799267, 'Apprehension': 0.0830330313557733, 'Sharpener': 0.3863878711824424}\n",
      "\n",
      "Most frequent words (at least in 50% of definitions) : \n",
      "[('Courage', ['fear', 'ability']), ('Paper', ['material', 'write']), ('Apprehension', []), ('Sharpener', ['pencil', 'sharpen', 'tool'])]\n"
     ]
    }
   ],
   "source": [
    "with open('definizioni.csv') as csvfile:\n",
    "    definitions_words = get_definitions(csvfile)\n",
    "    results = compute_results(definitions_words)\n",
    "    print('Average definition\\'s cosine similarity: ')\n",
    "    print(results)\n",
    "    print('\\nMost frequent words (at least in 50% of definitions) : ')\n",
    "    print([(key, most_frequent_words(definitions_words[key])) for key in ['Courage', 'Paper', 'Apprehension', 'Sharpener']])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "687777cbfed9bcee7b08ad56f15c17d8908988aca6cd0ae5484f3a9e4851bb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
