{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc20e482-4a22-4d78-84a4-58a62bfabfc9",
   "metadata": {},
   "source": [
    "## Exercize 2.1\n",
    "### Summarization using Nasari\n",
    "#### Francesco Sannicola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6485cd96-b8a1-45b5-bb0d-ac4228a302d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4546dcb-58e2-4b98-bad9-4dce0a01276f",
   "metadata": {},
   "source": [
    "Reads the NASARI file and calculate a dict as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e56a58-c1a5-46c0-90b1-0f8b173dbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nasari(file):\n",
    "    \"\"\"\n",
    "    :return: a dictionary as {word: {{term:score},...}}\n",
    "    \"\"\"\n",
    "    nasari = {}\n",
    "    with open(file, 'r', encoding=\"utf8\") as file:\n",
    "        for row in file.readlines():\n",
    "            line_splitted = row.split(\";\")\n",
    "            dict_entry = {}\n",
    "            \n",
    "            # Start from 2 letter (delete \"bn:\")\n",
    "            for term in line_splitted[2:]:\n",
    "                # term and score written like this: \"serotonin_1841.0\"\n",
    "                term_score = term.split(\"_\")\n",
    "                if len(term_score) > 1:\n",
    "                    dict_entry[term_score[0]] = term_score[1]\n",
    "\n",
    "            nasari[line_splitted[1].lower()] = dict_entry\n",
    "\n",
    "    return nasari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5524a05-467b-41fb-be83-60af0aee0b21",
   "metadata": {},
   "source": [
    "Reads the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f42c7b-d0cb-413a-95f2-804b4bdc3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(file):\n",
    "    \"\"\"\n",
    "    :param file: document's path\n",
    "    :return: a list of all paragraph.\n",
    "    \"\"\"\n",
    "    document = []\n",
    "    with open(file, 'r', encoding=\"utf8\") as file:\n",
    "        for row in file.readlines():\n",
    "            # does not consider lines starting with \"#\"\n",
    "            if '#' not in row:\n",
    "                row = row[:-1]\n",
    "                if row != '':\n",
    "                    document.append(row)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c5eb3-132b-4f8e-90fe-aad27ba121ac",
   "metadata": {},
   "source": [
    "Computes the rank of the given vector. Method used to calculate the weighted overlap between nasari vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd63962-b056-40b1-8e60-6ae3a5a3d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank(vector, nasari_vector):\n",
    "    \"\"\"\n",
    "    :param vector: input vector\n",
    "    :param nasari_vector: input Nasari vector\n",
    "    :return: vector's rank (position inside the nasari_vector)\n",
    "    \"\"\"\n",
    "\n",
    "    j=9\n",
    "    for i in range(len(nasari_vector)):\n",
    "        if nasari_vector[i] == vector:\n",
    "            # returns index of nasari_vector egual to input vector\n",
    "            return i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74a2b3-11d6-4e0d-b9b3-e867c8da0ffc",
   "metadata": {},
   "source": [
    "Implementation of the Weighted Overlap between two nasari vectors.\n",
    "$$\n",
    "  WO(w_1,w_2) = \\frac{\\sum_{q \\in O} (rank(q, v1) + rank(q, v2))^{-1}}{\\sum_{i=1}^{|O|} (2i)^{-1}}\n",
    "$$\n",
    "\n",
    "More is WO and more will be similar that 2 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdef25fe-2e8a-4dce-8557-d94a588b5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_overlap(nasari_vector_1, nasari_vector_2):\n",
    "    \"\"\"\n",
    "    :param nasari_vector_1: in our case represents the Nasari of topic\n",
    "    :param nasari_vector_2: in our case represents the Nasari of paragraph\n",
    "    :return: Weighted Overlap square-rooted of two nasari vectors\n",
    "    \"\"\"\n",
    "\n",
    "    overlap_keys = nasari_vector_1.keys() & nasari_vector_2.keys()\n",
    "\n",
    "    list_overlap_keys = list(overlap_keys)\n",
    "\n",
    "    if len(list_overlap_keys) != 0:\n",
    "        return (sum(1 / (calculate_rank(vector, list(nasari_vector_1)) + calculate_rank(vector, list(nasari_vector_2))) for vector in list_overlap_keys)) \\\n",
    "               / \\\n",
    "               (sum(list(map(lambda x: 1 / (2 * x), list(range(1, len(list_overlap_keys) + 1))))))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fea36-6b13-4650-9890-997e93f9ad28",
   "metadata": {},
   "source": [
    "A bag of word algorithm based approach. It calculates a list of word given a text doing stop word and punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97946242-12b9-4e0f-a4dd-6698e94c9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_word_approach(text):\n",
    "    \"\"\"\n",
    "    :param text: input text\n",
    "    :return: BoW representation of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    # text tokenizzation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove stop_word and punctuation\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in string.punctuation, tokens))\n",
    "    \n",
    "    return set(wordnet_lemmatizer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711ac31-8d40-4435-b4a8-a7a55bdc6c72",
   "metadata": {},
   "source": [
    "Create a list of nasari vectors depending on the document title (topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4a4590-8f41-4492-8fef-c1c027ed6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_from_title(document, nasari):\n",
    "    \"\"\"\n",
    "    :param document: input document\n",
    "    :param nasari: Nasari dictionary\n",
    "    :return: a list of Nasari vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # title in on the first document entry\n",
    "    title = document[0]\n",
    "    \n",
    "    # topic calculated with BOW approach\n",
    "    topic = bag_of_word_approach(title)\n",
    "\n",
    "    # NB nasari_vectors is a dict of dicts {word: {{term:score},...}}\n",
    "    nasari_vectors = []\n",
    "\n",
    "    for word in topic:\n",
    "        if word in nasari.keys():\n",
    "            nasari_vectors.append(nasari[word])\n",
    "\n",
    "    return nasari_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24591d8e-d92d-468d-93fa-7844e531065a",
   "metadata": {},
   "source": [
    "Create a list of nasari vectors depending of text's terms. Very similar to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2769748-b5fb-4959-a9f9-52a4f0cd5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_nasari(text, nasari):\n",
    "    \"\"\"\n",
    "    :param text: the list of text's terms\n",
    "    :param nasari: Nasari dictionary\n",
    "    :return: list of Nasari's vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = bag_of_word_approach(text)\n",
    "    \n",
    "    nasari_vectors = []\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in nasari.keys():\n",
    "            nasari_vectors.append(nasari[word]) \n",
    "\n",
    "    return nasari_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb390aff-d337-4e5f-8335-e1b3086fda61",
   "metadata": {},
   "source": [
    "Given a list of paragraph from a document, calculate how many of these are preserved depending to a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a5084f-feb9-49c5-8c11-d69688207bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lines_to_keep(doc_paragraphs, percentage):\n",
    "    \"\"\"\n",
    "    :param doc_paragraphs: document's paragraphs as a list\n",
    "    :param percentage: reduction percentage\n",
    "    :return: number of paragraphs to preserve\n",
    "    \"\"\"\n",
    "    return len(doc_paragraphs) - int(round((percentage / 100) * len(doc_paragraphs), 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fc2a0-1860-4a19-94d0-3d3918d7a50e",
   "metadata": {},
   "source": [
    "Given a list of paragraphs annotated with overlap scores, compute the summarized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c392e1-344e-40ea-a3f1-0aac863fa66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_document(doc_paragraphs_overlaps, lines_to_keep):\n",
    "    \"\"\"\n",
    "    :param doc_paragraphs_overlaps: document's paragraphs as a list with an overlap score\n",
    "    :param lines_to_keep: number of paragraph to keep\n",
    "    :return: reduced document\n",
    "    \"\"\"\n",
    "    # Order by weighted overlap\n",
    "    document_sorted  = sorted(doc_paragraphs_overlaps, key=lambda x: x[1], reverse=True)\n",
    "    reduced_document = document_sorted[:lines_to_keep]\n",
    "    \n",
    "    # Restore original order\n",
    "    reduced_document = sorted(reduced_document, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Obtain the text\n",
    "    reduced_document_text = list(map(lambda x: x[2], reduced_document))\n",
    "    \n",
    "    # Add the title\n",
    "    reduced_document_text = [document[0]] + reduced_document_text\n",
    "    \n",
    "    return reduced_document_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bdc07-454e-4868-9e28-39f84435602e",
   "metadata": {},
   "source": [
    "Applies summarization to the given document, with the specific redution percentage.\n",
    "I will use title approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c88a73-1963-4a61-ab85-b40a2011255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(document, nasari, percentage):\n",
    "    \"\"\"\n",
    "    :param document: document\n",
    "    :param nasari_dict: nasari dict of dicts\n",
    "    :param percentage: reduction percentage\n",
    "    :return: document summarized\n",
    "    \"\"\"\n",
    "\n",
    "    # First step: calculate topic/topics from title\n",
    "    topics = get_topic_from_title(document, nasari)\n",
    "    doc_paragraphs = []\n",
    "    i = 0\n",
    "    \n",
    "    for doc_paragraph in document[1:]:\n",
    "        \n",
    "        # obtain nasari rappresentation of the paragraph\n",
    "        nasari_text_par = text_to_nasari(doc_paragraph, nasari)\n",
    "        paragraph_weighted_overlap = 0\n",
    "        \n",
    "        # word is a nasari rappresentation of the term {word: {{term:score},...}}\n",
    "        for word in nasari_text_par:\n",
    "            topic_weighted_overlap = 0\n",
    "            \n",
    "            for topic in topics:\n",
    "                # for each topic compute the WO for topic and word (comulative)\n",
    "                topic_weighted_overlap += weighted_overlap(word, topic)\n",
    "            \n",
    "            # Mean of WO (based on number of topic)\n",
    "            if topic_weighted_overlap != 0:\n",
    "                topic_weighted_overlap /= len(topics)\n",
    "            \n",
    "            \n",
    "            # Comulative paragraph's WO\n",
    "            paragraph_weighted_overlap += topic_weighted_overlap\n",
    "\n",
    "        if len(nasari_text_par) != 0:\n",
    "            # Mean of paragraph's WO\n",
    "            paragraph_weighted_overlap /= len(nasari_text_par)\n",
    "            # Create a tuple with paragraph's number, WO and text. Append it.\n",
    "            doc_paragraphs.append((i, paragraph_weighted_overlap, doc_paragraph))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Obtain number of lines to keep\n",
    "    lines_to_keep = calculate_lines_to_keep(doc_paragraphs, percentage)\n",
    "    \n",
    "    # Finally we can execute summarization\n",
    "    reduced_document = reduce_document(doc_paragraphs, lines_to_keep)\n",
    "    \n",
    "    return reduced_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ae58c-fa26-4b3d-a760-8e3412f3cc8e",
   "metadata": {},
   "source": [
    "Call all previously defined methods.\n",
    "Also calcule BLEU and ROUGE score to see how similar the results are compared to the original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bad6ba3-4a29-4576-b6ac-f57eb7577199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "\u001b[1mAndy-Warhol \u001b[0m\tOriginal lenght: 20\n",
      "10 % redution \tSummary lenght: 18\n",
      "\n",
      "BLEU score:  0.8948393168143697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thewp\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\thewp\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\thewp\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue scores:  [{'rouge-1': {'r': 0.8813559322033898, 'p': 1.0, 'f': 0.9369369319568218}, 'rouge-2': {'r': 0.8511415525114155, 'p': 0.982086406743941, 'f': 0.9119373727163127}, 'rouge-l': {'r': 0.8813559322033898, 'p': 1.0, 'f': 0.9369369319568218}}]\n",
      "20 % redution \tSummary lenght: 16\n",
      "\n",
      "BLEU score:  0.7788007830714049\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.7812018489984591, 'p': 1.0, 'f': 0.8771626248332306}, 'rouge-2': {'r': 0.7296803652968037, 'p': 0.9815724815724816, 'f': 0.837087475464543}, 'rouge-l': {'r': 0.7812018489984591, 'p': 1.0, 'f': 0.8771626248332306}}]\n",
      "30 % redution \tSummary lenght: 14\n",
      "\n",
      "BLEU score:  0.6514390575310556\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.724191063174114, 'p': 1.0, 'f': 0.8400357413299089}, 'rouge-2': {'r': 0.6621004566210046, 'p': 0.9823848238482384, 'f': 0.7910529139021558}, 'rouge-l': {'r': 0.724191063174114, 'p': 1.0, 'f': 0.8400357413299089}}]\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[1mEbola-virus-disease \u001b[0m\tOriginal lenght: 28\n",
      "10 % redution \tSummary lenght: 23\n",
      "\n",
      "BLEU score:  0.8046150583253528\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.9588100686498856, 'p': 1.0, 'f': 0.9789719576190334}, 'rouge-2': {'r': 0.943013698630137, 'p': 0.9885123492245835, 'f': 0.9652271402635708}, 'rouge-l': {'r': 0.9588100686498856, 'p': 1.0, 'f': 0.9789719576190334}}]\n",
      "20 % redution \tSummary lenght: 20\n",
      "\n",
      "BLEU score:  0.6703200460356393\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.9130434782608695, 'p': 1.0, 'f': 0.9545454495557851}, 'rouge-2': {'r': 0.873972602739726, 'p': 0.989454094292804, 'f': 0.9281349964739601}, 'rouge-l': {'r': 0.9130434782608695, 'p': 1.0, 'f': 0.9545454495557851}}]\n",
      "30 % redution \tSummary lenght: 18\n",
      "\n",
      "BLEU score:  0.5737534207374327\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.7871853546910755, 'p': 1.0, 'f': 0.8809218900773}, 'rouge-2': {'r': 0.749041095890411, 'p': 0.9884309472161966, 'f': 0.8522443841223489}, 'rouge-l': {'r': 0.7871853546910755, 'p': 1.0, 'f': 0.8809218900773}}]\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[1mLife-indoors \u001b[0m\tOriginal lenght: 13\n",
      "10 % redution \tSummary lenght: 12\n",
      "\n",
      "BLEU score:  0.9200444146293233\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.9548192771084337, 'p': 1.0, 'f': 0.9768875142630716}, 'rouge-2': {'r': 0.9324577861163227, 'p': 0.9822134387351779, 'f': 0.9566891191612207}, 'rouge-l': {'r': 0.9548192771084337, 'p': 1.0, 'f': 0.9768875142630716}}]\n",
      "20 % redution \tSummary lenght: 11\n",
      "\n",
      "BLEU score:  0.8337529180751805\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.8885542168674698, 'p': 1.0, 'f': 0.9409888307430895}, 'rouge-2': {'r': 0.8592870544090057, 'p': 0.9807280513918629, 'f': 0.91599999502178}, 'rouge-l': {'r': 0.8885542168674698, 'p': 1.0, 'f': 0.9409888307430895}}]\n",
      "30 % redution \tSummary lenght: 9\n",
      "\n",
      "BLEU score:  0.6411803884299546\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.7259036144578314, 'p': 1.0, 'f': 0.8411867316008029}, 'rouge-2': {'r': 0.6679174484052532, 'p': 0.9807162534435262, 'f': 0.7946428523228486}, 'rouge-l': {'r': 0.7259036144578314, 'p': 1.0, 'f': 0.8411867316008029}}]\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[1mNapoleon-wiki \u001b[0m\tOriginal lenght: 18\n",
      "10 % redution \tSummary lenght: 16\n",
      "\n",
      "BLEU score:  0.8824969025845955\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.8536585365853658, 'p': 0.997624703087886, 'f': 0.920043806640314}, 'rouge-2': {'r': 0.8507295173961841, 'p': 0.9831387808041504, 'f': 0.912154026313671}, 'rouge-l': {'r': 0.8536585365853658, 'p': 0.997624703087886, 'f': 0.920043806640314}}]\n",
      "20 % redution \tSummary lenght: 15\n",
      "\n",
      "BLEU score:  0.8187307530779819\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.7865853658536586, 'p': 0.9974226804123711, 'f': 0.8795454496152894}, 'rouge-2': {'r': 0.7598204264870931, 'p': 0.9825834542815675, 'f': 0.8569620203981814}, 'rouge-l': {'r': 0.7865853658536586, 'p': 0.9974226804123711, 'f': 0.8795454496152894}}]\n",
      "30 % redution \tSummary lenght: 13\n",
      "\n",
      "BLEU score:  0.6807123983233854\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.6788617886178862, 'p': 0.9970149253731343, 'f': 0.8077388101741556}, 'rouge-2': {'r': 0.6318742985409652, 'p': 0.9825479930191972, 'f': 0.769125678296017}, 'rouge-l': {'r': 0.6788617886178862, 'p': 0.9970149253731343, 'f': 0.8077388101741556}}]\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[1mTrump-wall \u001b[0m\tOriginal lenght: 63\n",
      "10 % redution \tSummary lenght: 51\n",
      "\n",
      "BLEU score:  0.7903383629814982\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.9522409992652462, 'p': 1.0, 'f': 0.9755363141599364}, 'rouge-2': {'r': 0.9195930423367247, 'p': 0.9855786141399929, 'f': 0.9514431189448774}, 'rouge-l': {'r': 0.9485672299779574, 'p': 0.996141975308642, 'f': 0.9717726709533124}}]\n",
      "20 % redution \tSummary lenght: 46\n",
      "\n",
      "BLEU score:  0.6910347152078063\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.8339456282145481, 'p': 1.0, 'f': 0.9094551232461201}, 'rouge-2': {'r': 0.7981621266819823, 'p': 0.9842169162282477, 'f': 0.8814787917199404}, 'rouge-l': {'r': 0.8302718589272594, 'p': 0.9955947136563876, 'f': 0.9054487129897099}}]\n",
      "30 % redution \tSummary lenght: 40\n",
      "\n",
      "BLEU score:  0.5627048688069557\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.6943423952975754, 'p': 1.0, 'f': 0.8196010359259454}, 'rouge-2': {'r': 0.6334099113882508, 'p': 0.9826883910386965, 'f': 0.7703053235113381}, 'rouge-l': {'r': 0.6914033798677444, 'p': 0.9957671957671957, 'f': 0.816131825171392}}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from bleu import multi_list_bleu\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "nasari_file = 'utils/NASARI_vectors/dd-small-nasari-15.txt'\n",
    "all_docs = ['utils/docs/Andy-Warhol.txt', 'utils/docs/Ebola-virus-disease.txt', 'utils/docs/Life-indoors.txt', 'utils/docs/Napoleon-wiki.txt', 'utils/docs/Trump-wall.txt']\n",
    "\n",
    "nasari = read_nasari(nasari_file)\n",
    "rouge = Rouge()\n",
    "\n",
    "\n",
    "for doc in all_docs:\n",
    "    \n",
    "    document = read_doc(doc)\n",
    "    print('--------------------------------------------------------------------')\n",
    "    print('\\033[1m' +doc.replace(\"utils/docs/\", \"\").replace(\".txt\", \"\"), '\\033[0m' + \"\\tOriginal lenght:\", len(document))\n",
    "    \n",
    "    for percentage_decrease in [10,20,30]:\n",
    "        \n",
    "        summary = summarization(document, nasari, percentage_decrease)\n",
    "\n",
    "        print(percentage_decrease, \"% redution\", \"\\tSummary lenght:\", len(summary))\n",
    "        print()\n",
    "        \n",
    "        # Compute BLEU only for 1-gram\n",
    "        print(\"BLEU score: \", sentence_bleu([document], summary, weights=(1, 0, 0, 0)))\n",
    "        \n",
    "        # COmpute rouge scores for unigram, bigram and l-gram. F1, precision and recall.\n",
    "        rouge_scores = rouge.get_scores(' '.join(summary), ' '.join(document))\n",
    "        print(\"Rogue scores: \", rouge_scores)\n",
    "        \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
