{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235ef10e-8b8e-48a1-9346-fca3cf2c235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a036e216-14b9-4bb9-bac0-8065d9c41b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "# return row of the .csv file\n",
    "def get_rows(readCSV):\n",
    "    for row in readCSV: return row\n",
    "\n",
    "# Union of the pre-processed words of the definitions and terms from the examples in WN for a sense.\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86073737-49bb-452b-b062-23894f7aba74",
   "metadata": {},
   "source": [
    "Takes the .csv definitions file as input and outputs a dictionary in which each word ('Courage', 'Paper', 'Apprehension', 'Sharpener')  is associated with a list of pre-processed definitions.\n",
    "\n",
    "Returns a dict with terms as keys and a list of words within  term's definitions as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fd0bb5-8a3a-4a53-b602-cc7083224467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_definitions(file):\n",
    "    readCSV = csv.reader(file, delimiter=',')\n",
    "    words = get_rows(readCSV)[1:] \n",
    "\n",
    "    definitions_words = dict()\n",
    "\n",
    "    for row in readCSV:\n",
    "        for index, definition in enumerate(row):\n",
    "             # check the definition is empty\n",
    "            if definition: \n",
    "                if index > 0:\n",
    "                    word = words[index - 1]\n",
    "                    if word not in definitions_words.keys():\n",
    "                        definitions_words[word] = pre_processing(definition)\n",
    "                    else:\n",
    "                        definitions_words[word].extend(pre_processing(definition))\n",
    "    return definitions_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f95daf-36f5-4ba6-9006-ab8a561bb178",
   "metadata": {},
   "source": [
    "Returns the list of the 5 most important (most frequent) terms that appear in the bag of words of the definitions of the concept expressed by the word 'concept_word'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1f8ce5-239a-4e14-86b2-3dee617961ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_num = 5  # the number of most important terms to extract for a concept.\n",
    "def get_genus(definitions_word):\n",
    "    dict_Counter = dict (Counter(definitions_word))\n",
    "    bag_of_words_count = []\n",
    "    for key in dict_Counter.keys():\n",
    "        bag_of_words_count.append((key, dict_Counter[key]))\n",
    "\n",
    "    return sorted(bag_of_words_count, key=lambda x: x[1], reverse=True)[:genus_num] # takes only first 5 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1b93e-5ca9-440c-aec9-65a9aff5b54c",
   "metadata": {},
   "source": [
    "For each genus, get a possible candidate. So for each genus get a list of hyponyms of all synsets related to the term genus.\n",
    "Return the synset hyponym that comes closest to the definitions of the concept_word, that is the one with the signature (gloss + examples) that have a better overlap with the definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8131b6c7-0c4d-4a18-a1e5-a8e91217434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(genus_list, definitions_word):\n",
    "    bag_of_words_defs = set(definitions_word)\n",
    "    candidates = []\n",
    "    for genus in genus_list:\n",
    "        hyponym_list = []\n",
    "        for synset in wn.synsets(genus[0]): # Obtain synsets (only for nouns) of genus word, We can also consider \n",
    "            hyponym_list.extend(synset.hyponyms())\n",
    "        if len(hyponym_list) != 0:\n",
    "            candidate = genus[0], hyponym_list[0]\n",
    "            max_overlap = 0\n",
    "            for hyponym in hyponym_list:\n",
    "                hyponym_signature = get_signature(hyponym)\n",
    "                overlap = len(list(set(hyponym_signature) & bag_of_words_defs))\n",
    "                if overlap > max_overlap:\n",
    "                    candidate = genus[0], hyponym\n",
    "                    max_overlap = overlap\n",
    "            candidates.append(candidate)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9921a075-644a-4b33-b45a-111595a45b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Concept:  Courage\n",
      "Wordnet synsets:  [Synset('courage.n.01')]\n",
      "\n",
      "Genus list (with frequency): \n",
      "[('ability', 18), ('fear', 17), ('face', 9), ('situation', 7), ('scar', 5)]\n",
      "\n",
      "Candidates:\n",
      "[('ability', Synset('physical_ability.n.01')), ('fear', Synset('stage_fright.n.01')), ('face', Synset('take_the_bull_by_the_horns.v.01')), ('situation', Synset('crowding.n.01')), ('scar', Synset('keloid.n.01'))]\n",
      "-----------------------\n",
      "Concept:  Paper\n",
      "Wordnet synsets:  [Synset('paper.n.01'), Synset('composition.n.08'), Synset('newspaper.n.01'), Synset('paper.n.04'), Synset('paper.n.05'), Synset('newspaper.n.02'), Synset('newspaper.n.03'), Synset('paper.v.01'), Synset('wallpaper.v.01')]\n",
      "\n",
      "Genus list (with frequency): \n",
      "[('material', 23), ('write', 18), ('cellulose', 7), ('wood', 6), ('tree', 5)]\n",
      "\n",
      "Candidates:\n",
      "[('material', Synset('composite_material.n.01')), ('write', Synset('handwrite.v.01')), ('cellulose', Synset('pulp.n.03')), ('wood', Synset('balsa.n.01')), ('tree', Synset('poon.n.02'))]\n",
      "-----------------------\n",
      "Concept:  Apprehension\n",
      "Wordnet synsets:  [Synset('apprehension.n.01'), Synset('understanding.n.01'), Synset('apprehension.n.03'), Synset('apprehension.n.04')]\n",
      "\n",
      "Genus list (with frequency): \n",
      "[('fear', 10), ('anxiety', 10), ('feeling', 5), ('happen', 5), ('feel', 4)]\n",
      "\n",
      "Candidates:\n",
      "[('fear', Synset('apprehension.n.01')), ('anxiety', Synset('panic.n.02')), ('feeling', Synset('glow.v.05')), ('happen', Synset('concur.v.02')), ('feel', Synset('glow.v.05'))]\n",
      "-----------------------\n",
      "Concept:  Sharpener\n",
      "Wordnet synsets:  [Synset('sharpener.n.01')]\n",
      "\n",
      "Genus list (with frequency): \n",
      "[('pencil', 25), ('sharpen', 17), ('tool', 16), ('object', 11), ('allow', 4)]\n",
      "\n",
      "Candidates:\n",
      "[('pencil', Synset('lead_pencil.n.01')), ('sharpen', Synset('edge.v.04')), ('tool', Synset('drill.n.01')), ('object', Synset('commemorative.n.01')), ('allow', Synset('pass.v.17'))]\n"
     ]
    }
   ],
   "source": [
    "with open('definizioni.csv') as file:\n",
    "        # Create a dict with terms as keys and a list of words within  term's defitions as a value\n",
    "\n",
    "        definitions_words = get_definitions(file)\n",
    "        for key in definitions_words.keys():\n",
    "            print(\"-----------------------\")\n",
    "            print(\"Concept: \",key)\n",
    "            print(\"Wordnet synsets: \", wn.synsets(key))\n",
    "            print()\n",
    "            print(\"Genus list (with frequency): \")\n",
    "            genus_list = get_genus(definitions_words[key])\n",
    "            print(genus_list)\n",
    "            print(\"\\nCandidates:\")\n",
    "            print(get_candidates(genus_list, definitions_words[key]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "687777cbfed9bcee7b08ad56f15c17d8908988aca6cd0ae5484f3a9e4851bb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
