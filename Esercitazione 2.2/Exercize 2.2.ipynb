{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3db477-5aaa-4665-837e-b7e500236e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aa620f-72b9-42ab-8ed7-30364228ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "# Union of the pre-processed words of the definitions and terms from the examples in WN for a sense.\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925cc97-00f4-44de-bbdf-20c5ebee3e01",
   "metadata": {},
   "source": [
    "Read corpus file. It return a list of list of documents and words for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4d1d86-387b-4b38-a1d8-9e12a3c75aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(txt_file):\n",
    "    \n",
    "    with open(txt_file, encoding='utf-8') as file:\n",
    "        # for each doc create list of pre-processed words in that doc (list of lists)\n",
    "        documents_words = []\n",
    "\n",
    "        for line in file:\n",
    "            if \"<doc\" in line:  # tag for new doc\n",
    "                document_words = []  # list of words that will be part of the document\n",
    "                while True:\n",
    "                    next_line = file.readline()  # read next line\n",
    "\n",
    "                    # remove unuseful tags\n",
    "                    next_line_proc = next_line.replace(\"<p> \", \"\").replace(\"</p>\\n\", \"\").replace(\"/p\", \"\")\n",
    "\n",
    "                    if \"</doc>\" in next_line:\n",
    "                        break\n",
    "                    # pre-processing steps\n",
    "                    sentence_words = pre_processing(next_line_proc)\n",
    "                    document_words.extend(sentence_words)\n",
    "                documents_words.append(document_words)\n",
    "        file.close()\n",
    "\n",
    "    print(\"Documents number: \", len(documents_words))\n",
    "    \n",
    "    return documents_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dced5e-4328-426d-a7a4-c0043841b2cb",
   "metadata": {},
   "source": [
    "Topic modelling using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732815ad-9ac4-4efc-ba4c-452675e0d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "topic_num = 10\n",
    "topic_words_num = 5\n",
    "\n",
    "def topic_modelling(documents_words):\n",
    "    \n",
    "    # Create a dict with integer keys for all words\n",
    "    dictionary_LDA = corpora.Dictionary(documents_words)\n",
    "\n",
    "    # delete all terms that do NOT appear in at least 3 documents.\n",
    "    # delete all terms that appear in more than 60% of documents (see filter_extremes official doc).\n",
    "    dictionary_LDA.filter_extremes(no_below=3, no_above=0.6)\n",
    "\n",
    "    # Converts each document into a list of BoW (list of (id_term, term_frequency) for each term in doc)\n",
    "    corpus_idbow_freq = [dictionary_LDA.doc2bow(document_words) for document_words in documents_words]\n",
    "    \n",
    "    # https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "    lda_model = models.LdaModel(corpus_idbow_freq, num_topics=topic_num, \\\n",
    "                                id2word=dictionary_LDA, \\\n",
    "                                passes=3, alpha=[0.01] * topic_num, \\\n",
    "                                eta=[0.01] * len(dictionary_LDA.keys()))\n",
    "    \n",
    "    return lda_model, corpus_idbow_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115948b8-8e87-43b2-8748-d51d60740723",
   "metadata": {},
   "source": [
    "Show topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657c372c-ce95-44c9-b774-358bd18a8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents number:  100\n",
      "Topic 0 : [('word', 0.029), ('travel', 0.027), ('learn', 0.023), ('language', 0.021), ('example', 0.013)]\n",
      "Topic 1 : [('lesson', 0.088), ('website', 0.023), ('pronunciation', 0.018), ('students', 0.016), ('life', 0.016)]\n",
      "Topic 2 : [('students', 0.069), ('exam', 0.035), ('book', 0.027), ('sb', 0.027), ('speak', 0.025)]\n",
      "Topic 3 : [('happen', 0.015), ('word', 0.014), ('clause', 0.014), ('example', 0.011), ('tense', 0.011)]\n",
      "Topic 4 : [('book', 0.025), ('hotel', 0.023), ('holiday', 0.015), ('beach', 0.014), ('lesson', 0.012)]\n",
      "Topic 5 : [('lesson', 0.039), ('student', 0.017), ('clause', 0.013), ('audio', 0.013), ('dailystep', 0.01)]\n",
      "Topic 6 : [('travel', 0.023), ('hotel', 0.021), ('place', 0.018), ('visit', 0.015), ('stay', 0.015)]\n",
      "Topic 7 : [('clause', 0.033), ('article', 0.018), ('level', 0.018), ('example', 0.017), ('video', 0.016)]\n",
      "Topic 8 : [('clause', 0.028), ('conditionals', 0.015), ('situation', 0.015), ('result', 0.015), ('happen', 0.014)]\n",
      "Topic 9 : [('students', 0.018), ('exam', 0.013), ('online', 0.012), ('travel', 0.012), ('__________________', 0.012)]\n"
     ]
    }
   ],
   "source": [
    "documents_words = read_corpus(\"travelling.txt\")\n",
    "\n",
    "model, corpus_idbow_freq = topic_modelling(documents_words)\n",
    "\n",
    "topics = {'Topic ' + str(i): [(token, round(score, 3)) for token, score in model.show_topic(i, topn=topic_words_num)] for i in range(0, model.num_topics)}\n",
    "for key, value in topics.items():\n",
    "    print(key,\":\", topics[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483113ca-cc75-42b0-989b-db18a485ea1a",
   "metadata": {},
   "source": [
    "Show topics for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3148513-5772-4920-b54c-322c1335b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents topic list\n",
      "Doc 0 : [(0, 0.41655), (4, 0.11179409), (5, 0.37715492), (6, 0.09350246)]\n",
      "Doc 1 : [(0, 0.31275022), (2, 0.31053978), (4, 0.37648568)]\n",
      "Doc 2 : [(0, 0.35156614), (8, 0.6465774)]\n",
      "Doc 3 : [(2, 0.14442235), (6, 0.8551233)]\n",
      "Doc 4 : [(5, 0.99982005)]\n",
      "Doc 5 : [(0, 0.99993885)]\n",
      "Doc 6 : [(4, 0.999012)]\n",
      "Doc 7 : [(5, 0.2981226), (8, 0.7012895)]\n",
      "Doc 8 : [(0, 0.18161604), (3, 0.083086155), (6, 0.69475526), (9, 0.040486548)]\n",
      "Doc 9 : [(0, 0.11459823), (8, 0.8851316)]\n",
      "Doc 10 : [(3, 0.56199676), (7, 0.041094378), (8, 0.39681107)]\n",
      "Doc 11 : [(3, 0.99972486)]\n",
      "Doc 12 : [(4, 0.052934803), (5, 0.8475884), (8, 0.09880422)]\n",
      "Doc 13 : [(5, 0.7594556), (7, 0.24025038)]\n",
      "Doc 14 : [(1, 0.999503)]\n",
      "Doc 15 : [(5, 0.03097208), (8, 0.53964955), (9, 0.42926398)]\n",
      "Doc 16 : [(3, 0.4124174), (7, 0.26083127), (8, 0.326196)]\n",
      "Doc 17 : [(6, 0.680636), (8, 0.25303105), (9, 0.06613853)]\n",
      "Doc 18 : [(5, 0.21412954), (8, 0.785572)]\n",
      "Doc 19 : [(7, 0.017388549), (8, 0.9822953)]\n",
      "Doc 20 : [(8, 0.9998232)]\n",
      "Doc 21 : [(8, 0.9996499)]\n",
      "Doc 22 : [(7, 0.9996566)]\n",
      "Doc 23 : [(8, 0.99968976)]\n",
      "Doc 24 : [(9, 0.9990228)]\n",
      "Doc 25 : [(8, 0.9998448)]\n",
      "Doc 26 : [(8, 0.9996897)]\n",
      "Doc 27 : [(3, 0.99836653)]\n",
      "Doc 28 : [(5, 0.36864576), (8, 0.6311203)]\n",
      "Doc 29 : [(5, 0.37689233), (8, 0.62287647)]\n",
      "Doc 30 : [(5, 0.9995003)]\n",
      "Doc 31 : [(0, 0.9994975)]\n",
      "Doc 32 : [(0, 0.9994975)]\n",
      "Doc 33 : [(1, 0.98652834), (5, 0.01183029)]\n",
      "Doc 34 : [(0, 0.020586519), (6, 0.02645484), (7, 0.95282704)]\n",
      "Doc 35 : [(8, 0.9996105)]\n",
      "Doc 36 : [(9, 0.9996387)]\n",
      "Doc 37 : [(8, 0.12674336), (9, 0.872978)]\n",
      "Doc 38 : [(4, 0.26871178), (8, 0.7301467)]\n",
      "Doc 39 : [(5, 0.93163216), (7, 0.043474246), (9, 0.01615673)]\n",
      "Doc 40 : [(6, 0.9994004)]\n",
      "Doc 41 : [(3, 0.99883264)]\n",
      "Doc 42 : [(4, 0.8889476), (8, 0.110945195)]\n",
      "Doc 43 : [(8, 0.99948305)]\n",
      "Doc 44 : [(5, 0.1595458), (8, 0.84002656)]\n",
      "Doc 45 : [(9, 0.99962354)]\n",
      "Doc 46 : [(3, 0.011913885), (5, 0.7157113), (9, 0.2722537)]\n",
      "Doc 47 : [(2, 0.99998856)]\n",
      "Doc 48 : [(7, 0.99980223)]\n",
      "Doc 49 : [(8, 0.99983424)]\n",
      "Doc 50 : [(4, 0.9999494)]\n",
      "Doc 51 : [(6, 0.99980223)]\n",
      "Doc 52 : [(0, 0.3653171), (4, 0.016820768), (8, 0.6068559), (9, 0.010814516)]\n",
      "Doc 53 : [(0, 0.111299835), (5, 0.7888576), (7, 0.04355662), (8, 0.048184898)]\n",
      "Doc 54 : [(5, 0.99894226)]\n",
      "Doc 55 : [(0, 0.66895825), (5, 0.33025807)]\n",
      "Doc 56 : [(0, 0.9994646)]\n",
      "Doc 57 : [(5, 0.9572581), (8, 0.041647397)]\n",
      "Doc 58 : [(4, 0.4142324), (5, 0.58266073)]\n",
      "Doc 59 : [(7, 0.99972147)]\n",
      "Doc 60 : [(4, 0.99948)]\n",
      "Doc 61 : [(3, 0.9927917)]\n",
      "Doc 62 : [(4, 0.9993435)]\n",
      "Doc 63 : [(0, 0.39193884), (4, 0.1325483), (9, 0.47519183)]\n",
      "Doc 64 : [(7, 0.99974144)]\n",
      "Doc 65 : [(7, 0.84038335), (8, 0.15712406)]\n",
      "Doc 66 : [(4, 0.9987854)]\n",
      "Doc 67 : [(4, 0.9985269)]\n",
      "Doc 68 : [(4, 0.9987854)]\n",
      "Doc 69 : [(7, 0.99960715)]\n",
      "Doc 70 : [(7, 0.18823901), (8, 0.8110342)]\n",
      "Doc 71 : [(5, 0.87868166), (8, 0.12086134)]\n",
      "Doc 72 : [(3, 0.25165138), (4, 0.5130329), (8, 0.23503006)]\n",
      "Doc 73 : [(3, 0.5434555), (8, 0.45563626)]\n",
      "Doc 74 : [(3, 0.8029663), (8, 0.19592398)]\n",
      "Doc 75 : [(5, 0.9998197)]\n",
      "Doc 76 : [(3, 0.053485993), (8, 0.94636714)]\n",
      "Doc 77 : [(0, 0.105759), (2, 0.1483968), (5, 0.63728225), (7, 0.030476151), (8, 0.07792236)]\n",
      "Doc 78 : [(8, 0.9993881)]\n",
      "Doc 79 : [(3, 0.9751473), (8, 0.024700914)]\n",
      "Doc 80 : [(1, 0.9996787)]\n",
      "Doc 81 : [(0, 0.6515038), (4, 0.31251884), (9, 0.035770908)]\n",
      "Doc 82 : [(5, 0.08693046), (7, 0.10215002), (9, 0.8101985)]\n",
      "Doc 83 : [(9, 0.99859583)]\n",
      "Doc 84 : [(0, 0.58592546), (4, 0.27012348), (6, 0.0396657), (8, 0.10414548)]\n",
      "Doc 85 : [(7, 0.09159295), (8, 0.90728164)]\n",
      "Doc 86 : [(7, 0.99967635)]\n",
      "Doc 87 : [(7, 0.9996267)]\n",
      "Doc 88 : [(0, 0.044935796), (4, 0.35753784), (8, 0.59648293)]\n",
      "Doc 89 : [(0, 0.45887655), (2, 0.24538328), (8, 0.29541612)]\n",
      "Doc 90 : [(4, 0.33964518), (8, 0.65884763)]\n",
      "Doc 91 : [(1, 0.9972808)]\n",
      "Doc 92 : [(3, 0.9481167), (8, 0.05121152)]\n",
      "Doc 93 : [(3, 0.9998171)]\n",
      "Doc 94 : [(7, 0.07390706), (8, 0.92512995)]\n",
      "Doc 95 : [(5, 0.9996155)]\n",
      "Doc 96 : [(5, 0.2818725), (8, 0.71778125)]\n",
      "Doc 97 : [(4, 0.16236255), (5, 0.34220243), (6, 0.05348492), (8, 0.4413256)]\n",
      "Doc 98 : [(7, 0.9996553)]\n",
      "Doc 99 : [(3, 0.10646901), (8, 0.89297956)]\n"
     ]
    }
   ],
   "source": [
    "print (\"Documents topic list\")\n",
    "for i in range (0, len(corpus_idbow_freq)):\n",
    "    print (\"Doc\", i, \":\", model[corpus_idbow_freq[i]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "687777cbfed9bcee7b08ad56f15c17d8908988aca6cd0ae5484f3a9e4851bb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
