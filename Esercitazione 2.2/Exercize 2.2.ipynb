{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3db477-5aaa-4665-837e-b7e500236e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30aa620f-72b9-42ab-8ed7-30364228ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "# Union of the pre-processed words of the definitions and terms from the examples in WN for a sense.\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925cc97-00f4-44de-bbdf-20c5ebee3e01",
   "metadata": {},
   "source": [
    "Read corpus file. It return a list of list of documents and words for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c4d1d86-387b-4b38-a1d8-9e12a3c75aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(txt_file):\n",
    "    \n",
    "    with open(txt_file, encoding='utf-8') as file:\n",
    "        # for each doc create list of pre-processed words in that doc (list of lists)\n",
    "        documents_words = []\n",
    "\n",
    "        for line in file:\n",
    "            if \"<doc\" in line:  # tag for new doc\n",
    "                document_words = []  # list of words that will be part of the document\n",
    "                while True:\n",
    "                    next_line = file.readline()  # read next line\n",
    "\n",
    "                    # remove unuseful tags\n",
    "                    next_line_proc = next_line.replace(\"<p> \", \"\").replace(\"</p>\\n\", \"\").replace(\"/p\", \"\")\n",
    "\n",
    "                    if \"</doc>\" in next_line:\n",
    "                        break\n",
    "                    # pre-processing steps\n",
    "                    sentence_words = pre_processing(next_line_proc)\n",
    "                    document_words.extend(sentence_words)\n",
    "                documents_words.append(document_words)\n",
    "        file.close()\n",
    "\n",
    "    print(\"Documents number: \", len(documents_words))\n",
    "    \n",
    "    return documents_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dced5e-4328-426d-a7a4-c0043841b2cb",
   "metadata": {},
   "source": [
    "Topic modelling using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732815ad-9ac4-4efc-ba4c-452675e0d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "topic_num = 10\n",
    "topic_words_num = 5\n",
    "\n",
    "def topic_modelling(documents_words):\n",
    "    \n",
    "    # Create a dict with integer keys for all words\n",
    "    dictionary_LDA = corpora.Dictionary(documents_words)\n",
    "\n",
    "    # delete all terms that do NOT appear in at least 3 documents.\n",
    "    # delete all terms that appear in more than 60% of documents (see filter_extremes official doc).\n",
    "    dictionary_LDA.filter_extremes(no_below=3, no_above=0.6)\n",
    "\n",
    "    # Converts each document into a list of BoW (list of (id_term, term_frequency) for each term in doc)\n",
    "    corpus_idbow_freq = [dictionary_LDA.doc2bow(document_words) for document_words in documents_words]\n",
    "    \n",
    "    # https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "    lda_model = models.LdaModel(corpus_idbow_freq, num_topics=topic_num, \\\n",
    "                                id2word=dictionary_LDA, \\\n",
    "                                passes=3, alpha=[0.01] * topic_num, \\\n",
    "                                eta=[0.01] * len(dictionary_LDA.keys()))\n",
    "    \n",
    "    return lda_model, corpus_idbow_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115948b8-8e87-43b2-8748-d51d60740723",
   "metadata": {},
   "source": [
    "Show topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657c372c-ce95-44c9-b774-358bd18a8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents number:  100\n",
      "Topic 0 : [('students', 0.024), ('good', 0.013), ('stay', 0.013), ('language', 0.012), ('exam', 0.012)]\n",
      "Topic 1 : [('clause', 0.023), ('word', 0.02), ('example', 0.015), ('result', 0.015), ('learn', 0.014)]\n",
      "Topic 2 : [('clause', 0.048), ('condition', 0.019), ('conditionals', 0.018), ('main', 0.018), ('happen', 0.018)]\n",
      "Topic 3 : [('students', 0.066), ('exam', 0.034), ('speak', 0.026), ('book', 0.026), ('sb', 0.026)]\n",
      "Topic 4 : [('book', 0.016), ('grammar', 0.014), ('hotel', 0.012), ('beach', 0.01), ('word', 0.009)]\n",
      "Topic 5 : [('clause', 0.02), ('happen', 0.019), ('study', 0.014), ('didnt', 0.012), ('help', 0.012)]\n",
      "Topic 6 : [('level', 0.018), ('clause', 0.017), ('esl', 0.016), ('teach', 0.016), ('student', 0.015)]\n",
      "Topic 7 : [('travel', 0.016), ('holiday', 0.016), ('clause', 0.009), ('place', 0.009), ('visit', 0.007)]\n",
      "Topic 8 : [('travel', 0.037), ('trip', 0.014), ('place', 0.013), ('money', 0.013), ('journey', 0.013)]\n",
      "Topic 9 : [('lesson', 0.051), ('audio', 0.012), ('hotel', 0.012), ('book', 0.011), ('clause', 0.01)]\n"
     ]
    }
   ],
   "source": [
    "documents_words = read_corpus(\"travelling.txt\")\n",
    "\n",
    "model, corpus_idbow_freq = topic_modelling(documents_words)\n",
    "\n",
    "topics = {'Topic ' + str(i): [(token, round(score, 3)) for token, score in model.show_topic(i, topn=topic_words_num)] for i in range(0, model.num_topics)}\n",
    "for key, value in topics.items():\n",
    "    print(key,\":\", topics[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483113ca-cc75-42b0-989b-db18a485ea1a",
   "metadata": {},
   "source": [
    "Show topics for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3148513-5772-4920-b54c-322c1335b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents topic list\n",
      "Doc 0 : [(8, 0.99850225)]\n",
      "Doc 1 : [(3, 0.12565248), (7, 0.87409115)]\n",
      "Doc 2 : [(2, 0.11221407), (9, 0.8859294)]\n",
      "Doc 3 : [(3, 0.13379017), (4, 0.7342464), (9, 0.13156591)]\n",
      "Doc 4 : [(9, 0.99982005)]\n",
      "Doc 5 : [(1, 0.94413257), (3, 0.055813007)]\n",
      "Doc 6 : [(7, 0.99901205)]\n",
      "Doc 7 : [(1, 0.9993387)]\n",
      "Doc 8 : [(7, 0.13536003), (8, 0.86456543)]\n",
      "Doc 9 : [(4, 0.022431323), (7, 0.8822507), (8, 0.069938906), (9, 0.025176316)]\n",
      "Doc 10 : [(1, 0.8725095), (2, 0.1184589)]\n",
      "Doc 11 : [(5, 0.048307106), (6, 0.85210794), (8, 0.09937094)]\n",
      "Doc 12 : [(4, 0.9991354)]\n",
      "Doc 13 : [(6, 0.9996692)]\n",
      "Doc 14 : [(3, 0.019953169), (7, 0.95620304), (9, 0.023457186)]\n",
      "Doc 15 : [(3, 0.023912033), (4, 0.26207802), (6, 0.71389556)]\n",
      "Doc 16 : [(1, 0.9992862)]\n",
      "Doc 17 : [(7, 0.99975)]\n",
      "Doc 18 : [(2, 0.7755761), (5, 0.16119501), (6, 0.06296783)]\n",
      "Doc 19 : [(1, 0.8710723), (5, 0.1286116)]\n",
      "Doc 20 : [(1, 0.3518342), (5, 0.64800864)]\n",
      "Doc 21 : [(1, 0.031995986), (2, 0.9676928)]\n",
      "Doc 22 : [(1, 0.9996566)]\n",
      "Doc 23 : [(5, 0.99968976)]\n",
      "Doc 24 : [(4, 0.8896731), (5, 0.10945815)]\n",
      "Doc 25 : [(5, 0.9998448)]\n",
      "Doc 26 : [(5, 0.36893776), (7, 0.6307864)]\n",
      "Doc 27 : [(1, 0.7501433), (5, 0.122489005), (8, 0.12609713)]\n",
      "Doc 28 : [(2, 0.9997369)]\n",
      "Doc 29 : [(2, 0.99973994)]\n",
      "Doc 30 : [(2, 0.18937856), (9, 0.8101772)]\n",
      "Doc 31 : [(8, 0.9994975)]\n",
      "Doc 32 : [(8, 0.9994975)]\n",
      "Doc 33 : [(9, 0.9999025)]\n",
      "Doc 34 : [(1, 0.9998308)]\n",
      "Doc 35 : [(5, 0.9996105)]\n",
      "Doc 36 : [(4, 0.9996387)]\n",
      "Doc 37 : [(4, 0.13214004), (7, 0.8675813)]\n",
      "Doc 38 : [(4, 0.6295154), (5, 0.06591566), (6, 0.1313735), (7, 0.17233934)]\n",
      "Doc 39 : [(1, 0.48953065), (3, 0.17010842), (4, 0.106956765), (6, 0.077795826), (8, 0.020675743), (9, 0.1348726)]\n",
      "Doc 40 : [(7, 0.61506444), (8, 0.3844025)]\n",
      "Doc 41 : [(5, 0.99883264)]\n",
      "Doc 42 : [(1, 0.9116181), (4, 0.015833566), (8, 0.051860977), (9, 0.020606909)]\n",
      "Doc 43 : [(1, 0.8894425), (5, 0.11009788)]\n",
      "Doc 44 : [(4, 0.0887339), (5, 0.07313365), (7, 0.8377583)]\n",
      "Doc 45 : [(4, 0.99962354)]\n",
      "Doc 46 : [(3, 0.08523775), (4, 0.19552846), (9, 0.7191127)]\n",
      "Doc 47 : [(3, 0.99998856)]\n",
      "Doc 48 : [(1, 0.56676614), (4, 0.43305802)]\n",
      "Doc 49 : [(5, 0.86788243), (6, 0.12108424), (7, 0.010904379)]\n",
      "Doc 50 : [(4, 0.8507449), (9, 0.1492101)]\n",
      "Doc 51 : [(0, 0.2509752), (8, 0.6014416), (9, 0.14742936)]\n",
      "Doc 52 : [(5, 0.024253776), (6, 0.97549075)]\n",
      "Doc 53 : [(1, 0.39218098), (4, 0.3936636), (6, 0.21402809)]\n",
      "Doc 54 : [(4, 0.9989423)]\n",
      "Doc 55 : [(8, 0.99911845)]\n",
      "Doc 56 : [(6, 0.7579761), (8, 0.24154787)]\n",
      "Doc 57 : [(0, 0.9987688)]\n",
      "Doc 58 : [(8, 0.6334084), (9, 0.36637303)]\n",
      "Doc 59 : [(6, 0.99972147)]\n",
      "Doc 60 : [(7, 0.99948)]\n",
      "Doc 61 : [(1, 0.669384), (5, 0.26570746), (7, 0.035757106), (9, 0.029111683)]\n",
      "Doc 62 : [(0, 0.9993435)]\n",
      "Doc 63 : [(7, 0.50451946), (8, 0.24014962), (9, 0.25500983)]\n",
      "Doc 64 : [(4, 0.21165024), (6, 0.7881199)]\n",
      "Doc 65 : [(6, 0.9971959)]\n",
      "Doc 66 : [(8, 0.03619252), (9, 0.9627277)]\n",
      "Doc 67 : [(9, 0.9985268)]\n",
      "Doc 68 : [(8, 0.03233106), (9, 0.9665892)]\n",
      "Doc 69 : [(6, 0.99960715)]\n",
      "Doc 70 : [(2, 0.19468993), (4, 0.61904144), (5, 0.045072313), (8, 0.14065121)]\n",
      "Doc 71 : [(1, 0.37607056), (4, 0.17900896), (8, 0.36971644), (9, 0.074861296)]\n",
      "Doc 72 : [(7, 0.9996328)]\n",
      "Doc 73 : [(5, 0.37586802), (7, 0.11006326), (9, 0.513274)]\n",
      "Doc 74 : [(5, 0.7425607), (7, 0.2563296)]\n",
      "Doc 75 : [(9, 0.99981964)]\n",
      "Doc 76 : [(2, 0.02283976), (4, 0.9208861), (5, 0.056145728)]\n",
      "Doc 77 : [(1, 0.12843066), (3, 0.05555906), (4, 0.79461783), (8, 0.02119636)]\n",
      "Doc 78 : [(1, 0.090662), (5, 0.41881374), (9, 0.4900483)]\n",
      "Doc 79 : [(1, 0.086352244), (5, 0.38191128), (7, 0.025381733), (8, 0.50624084)]\n",
      "Doc 80 : [(7, 0.9996787)]\n",
      "Doc 81 : [(8, 0.9997346)]\n",
      "Doc 82 : [(4, 0.4670815), (8, 0.5320945)]\n",
      "Doc 83 : [(8, 0.9985959)]\n",
      "Doc 84 : [(8, 0.7027622), (9, 0.2970513)]\n",
      "Doc 85 : [(1, 0.08007995), (8, 0.91879475)]\n",
      "Doc 86 : [(1, 0.9701167), (6, 0.029595599)]\n",
      "Doc 87 : [(1, 0.9996267)]\n",
      "Doc 88 : [(8, 0.9986586)]\n",
      "Doc 89 : [(8, 0.9995835)]\n",
      "Doc 90 : [(0, 0.0933556), (8, 0.90513736)]\n",
      "Doc 91 : [(2, 0.9972808)]\n",
      "Doc 92 : [(1, 0.7436383), (2, 0.12732069), (4, 0.12845315)]\n",
      "Doc 93 : [(5, 0.25055873), (8, 0.7492787)]\n",
      "Doc 94 : [(1, 0.27860025), (2, 0.6662197), (6, 0.054337498)]\n",
      "Doc 95 : [(6, 0.08997995), (9, 0.9096782)]\n",
      "Doc 96 : [(1, 0.029776325), (2, 0.15109612), (4, 0.02564435), (9, 0.7932235)]\n",
      "Doc 97 : [(9, 0.99906343)]\n",
      "Doc 98 : [(1, 0.81502706), (6, 0.18466653)]\n",
      "Doc 99 : [(5, 0.15443836), (6, 0.36183587), (8, 0.47682035)]\n"
     ]
    }
   ],
   "source": [
    "print (\"Documents topic list\")\n",
    "for i in range (0, len(corpus_idbow_freq)):\n",
    "    print (\"Doc\", i, \":\", model[corpus_idbow_freq[i]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "687777cbfed9bcee7b08ad56f15c17d8908988aca6cd0ae5484f3a9e4851bb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
